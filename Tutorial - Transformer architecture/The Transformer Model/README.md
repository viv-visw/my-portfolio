# The Transformer Model


## Overview

The [Transformer Model](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) is based on the idea of applying "attention" to only a handful of details to complete a particular task while discarding the rest.   

Conceptually, it is based on the idea of mimicking the human cognitive process of selectively focusing only on specific parts of any information resource, compared to the rest, to solve a problem. From machine translation to natural language processing tasks and beyond, Transformer models have come to redefine the state-of-the-art across several machine learning (ML) research domains and continue to enable spectacular breakthroughs within the broader artificial intelligence community. Therefore, this project aims to explain:

* what the Transformer model is
* how the Transformer model works
* all important details you would need to gain a useful intuition
* some technical implmementations to solve specific real-world tasks


## Scope

This project aims to answer the following questions about the Transformer model:

* What is the Transformer model?

* Why is it more successful than traditional sequence-to-sequence (seq2seq) deep learning models?

* How does the theoretical deisgn of the Transformer model ensure improved learning, in comparison to traditional deep recurrent neural networks?

* How can I use the Transformer model to solve a particular task?



## Outcomes 

At the end of this documentation, you will have a clear understanding of:

* what the Transformer model is and why it is significant
* why this model performs better than traditional seq2seq deep learning models
* how you can build this model for your task



## Target Audience

This documentation is meant for you, if you are trying to:
* understand how attention models work.
* understand the underlying mathematics underpinning attention models.
* understand how attention models can be implemented.
* design novel solutions to natural language processing tasks.
* learn more about the state-of-the-art deep learning research ideas.


## Prerequisites

This documentation assumes:

* You already have a background in machine learning and deep learning. 

* You understand the mathematical basics of deep neural networks, especially:

  * linear algebra

  * differential calculus

* You understand how recurrent neural networks work.

* You understand the basics of Natural Language Processing like vectorisation, tokenisation, and embeddings.

* You have prior knowledge about seq2seq models.

* You understand the basics of the attention mechanism.

* You have prior experience with computer programming in Python.

* You have prior experience with deep learning libraries such as Keras and TensorFlow.


## Table of Contents

| Topics | Links |
| :------:| :-----: |
| Getting Started with Transformers| [Getting Started with Transformers](Getting%20Started.md)
| How To Implement Transformers for Machine Translation | [Transformers for Machine Translation](How%20to%20use%20Transformers%20for%20Translation.md)
| Theory of Transformers| [Transformer Fundamentals](Important%20Concepts.md)
| References & Further Reading | [References & Further Reading](References%20and%20Further%20Reading.md)


## Author

For any queries, please contact:  

**Vivek Viswanath**
