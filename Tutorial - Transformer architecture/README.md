# Product Documentation  


## Overview

This project showcases my experience, understanding, and _general_ preference of creating an information architecture for a complex product and documenting its features and capabilities for different audiences.


## Highlights

This documentation describes the [Transformer model](The%20Transformer%20Model/README.md), a popular deep learning architecture that's been applied to a wide variety of machine learning and software applications. 

The following sections provide more information on how these models function, how they are used, and relevant resources for further reading:

- [Prerequisites](The%20Transformer%20Model/prerequisites.md)
- [Getting Started](The%20Transformer%20Model/getting-started.md)
- [How-To Guides](The%20Transformer%20Model/how-to-use-transformers-for-translation.md)
- [Important Concepts](The%20Transformer%20Model/important-concepts.md)


## Author Notes

- I have only used **Markdown** for this documentation (and no other tools).

- I have used **British English** standards and grammar for this project.

    - NOTE â€” I can create product documentation conforming to different English standards, as per business need.

- The documentation would look different had I used other tools such as Hugo, Jupyter Notebooks, CSS, or Sphinx, especially the following aspects:

  - Hugo shortcodes

  - CSS for customized look and feel.

  - Table of contents using the Sphinx-style `toctree`.

  - and so on.


## References

This page lists all the different resources I used to understand the Transformer architecture and create this documentation.


### Attention Models

- Sequence-to-Sequence Models: [Sutskever et al. 2014](https://arxiv.org/abs/1409.3215)

- Attention Paper: [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)

- Blog: [Sequence-to-Sequence Models in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)

- Blog: [The Attention Mechanism From Scratch](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/).


### Transformer

- Transformer Paper: ["Attention Is All You Need", Vaswani et al. 2017](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

- Book: [Francois Chollet. 2021. Deep Learning with Python (2nd. ed.). Manning Publications Co., USA.](https://www.manning.com/books/deep-learning-with-python-second-edition)

- Courses:

  - [Introduction to Deep Learning (STAT453) by Sebastian Raschka](https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks)

  - [Stanford CS25: Transformers United](https://web.stanford.edu/class/cs25/)  

- Blog: [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/#whats-wrong-with-seq2seq-model)


### Natural Language Processing

- Stanford Course: [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)

- Information Retrieval @ Stanford: [CS 276 / LING 286: Information Retrieval and Web Search](https://web.stanford.edu/class/cs276/)


### Machine Learning

Stanford Course: [CS229: Machine Learning](https://cs229.stanford.edu/)


## Author

**Vivek Viswanath**
