# Retrieval Augmented Generation


## Overview

The [Retrieval Augmented Generation (RAG)](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) model aims to enhance the quality of the output of a language generation model by incorporating external knowledge in its pipeline. This allows the model to improve the retrieval step that retrieves relevant context that ultimately conditions the LLM and augment its knowledge base. Essentially, the RAG model ensures finding relevant information from large corpora and use it to inform the generation process.

Conceptually, it is based on the idea of mimicking the human cognitive process of selectively focusing only on specific parts of any information resource, compared to the rest, to solve a problem. From machine translation to natural language processing tasks and beyond, Transformer models have come to redefine the state-of-the-art across several machine learning (ML) research domains and continue to enable spectacular breakthroughs within the broader artificial intelligence community. Therefore, this project aims to explain:

* what the Transformer model is
* how the Transformer model works
* all important details you would need to gain a useful intuition
* some technical implmementations to solve specific real-world tasks


## Scope

This project aims to answer the following questions about the Transformer model:

* What is the Transformer model?
* Why is it more successful than traditional sequence-to-sequence (seq2seq) deep learning models?
* How does the theoretical deisgn of the Transformer model ensure improved learning, in comparison to traditional deep recurrent neural networks?
* How can I use the Transformer model to solve a particular task?


## Outcomes 

At the end of this documentation, you will have a clear understanding of:

* what the Transformer model is and why it is significant
* why this model performs better than traditional seq2seq deep learning models
* how you can build this model for your task


## Target Audience

This documentation is meant for readers interested in/working on:
* understanding how attention models work
* understanding the underlying mathematics underpinning attention models
* understanding how attention models can be implemented
* novel solutions to natural language processing tasks
* latest deep learning research ideas


## Prerequisites

This documentation assumes:

* You already have a background in machine learning and deep learning. 
* You understand the mathematical basics of deep neural networks, especially:
  * linear algebra
  * differential calculus
* You understand how recurrent neural networks work.
* You understand the basics of Natural Language Processing like vectorisation, tokenisation, and embeddings.
* You have prior knowledge about seq2seq models.
* You understand the basics of the attention mechanism.
* You have prior experience with computer programming in Python.
* You have prior experience with deep learning libraries such as Keras and TensorFlow.


## Table of Contents

TBD

  
## Author

For any queries, please contact:  

**Vivek Viswanath**  





