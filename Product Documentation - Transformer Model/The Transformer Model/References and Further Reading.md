# References & Further Reading

This page lists all the different resources you would need to correctly and effectively understand the Transformer architecture and all its constituent building blocks or if you are interested in learning more about the Transformer model and its many variants for different applications.

## Attention Models

* Sequence-to-Sequence Models: [Sutskever et al. 2014](https://arxiv.org/abs/1409.3215)

* Attention Paper: [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)

* Blog: [Sequence-to-Sequence Models in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)

* Blog: [The Attention Mechanism From Scratch](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/).


## Transformer

* Transformer Paper: ["Attention Is All You Need", Vaswani et al. 2017](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

* Book: [Francois Chollet. 2021. Deep Learning with Python (2nd. ed.). Manning Publications Co., USA.](https://www.manning.com/books/deep-learning-with-python-second-edition)

* Course: [Introduction to Deep Learning (STAT453) by Sebastian Raschka](https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks)

* Blog: [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/#whats-wrong-with-seq2seq-model)

## Natural Language Processing

Stanford Course: [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)

Information Retrieval @ Stanford: [CS 276 / LING 286: Information Retrieval and Web Search](https://web.stanford.edu/class/cs276/)

## Machine Learning

Stanford Course: [CS229: Machine Learning](https://cs229.stanford.edu/)